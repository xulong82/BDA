---
title: "Why the prediction probabilities in Atezo were overwhemly low?"
output: html_document
---

## Preface

In modeling results of the Atezo project, we saw low prediction probabilities in the testing subjects, and consequently very few people were classified as cases by using 0.5 as the threshold. This is eye-itching at the first glance, in that proportion of cases in the training data was much higher. 

In order to understand this better, GNS team digged in the theoretical aspect of logistic regression model, along with simulation studies. This essay summarizes what we found.

Specifically, we want to address the following key questions:
1. Where the prediction probabilities come from? 
2. What do they mean?
3. What does a prediction probability of 0.5 mean?
4. Why the prediction probabilities in Atezo were overwhelmly low?
5. How imbalanced response affects logistic regression?
6. Is there a better threshold for classification?

```{r}
library(dplyr)
rm(list = ls())
```

REFS uses logistic regression model for binary outcomes (0 or 1). To start teh deep-diving, let's quickly go over the mathematics of logistic regression models.

In short, logistic regression models fit P(1) by linear combination of predictors via a link function.

$ P(1) = 1 / (1 + exp(-z)) $
$ z = b0 + b1 * x $

And the link function is as follow:

```{r}
pfun = function(z) 1/(1+exp(-z))
z = rnorm(1e2, 0, 3)
plot(z, pfun(z))
abline(h = 0.5)
abline(v = 0.0)
```

This is the sigmoid function. It is appealing because it turns all real numbers to [0, 1] with a smooth curve.

With the sigmoid function, prediction probability of each subject is all up to the model parameters (b0, b1), which were further estimated on the basis of the cost function. 

Cost function of logistic model is defined as the product of pfun(z) and (1-pfun(z)) for each subject.
We search for the best b0 and b1 that maximize the cost function.

Let's put the above concept into practice by using real life data. We choose a data in the "mlbench" package for illustration.

```{r}
library(mlbench)
data(BreastCancer, package="mlbench")

bc <- BreastCancer[complete.cases(BreastCancer), ] 
for(i in 2:10) bc[, i] <- as.numeric(as.character(bc[, i]))
bc$Class <- ifelse(bc$Class == "malignant", 1, 0)
bc$Class <- factor(bc$Class, levels = c(0, 1))
```

Let's stratify the subjects the response status: case or control

```{r}
bc0 = bc[bc$Class == 0, ] 
bc1 = bc[bc$Class == 1, ] 
```

## What if all subjects were 0

```{r}
fit <- glm(Class ~ Cell.shape, family = "binomial", data=bc0)
predict(fit, type = "response") %>% hist
```

The cost function does not converge, because it becomes monotonical and there is no optimal point to search for.
Also, prediction probabilities are all extremely low, which actually fit the data and the cost function.

## what if adding a few cases

```{r}
data = rbind(bc0, bc1[sample(1:nrow(bc1), 3), ])
fit <- glm(Class ~ Cell.shape, family = "binomial", data=data)
summary(fit)
(fit.p = predict(fit, type = "response")) %>% hist
table(fit.p > .5)
summary(fit.p[fit.p < .5])
```

Prediction probabilities are still very low for most subjects. Note that final cost is in fact the product of the prediction probabilities. Given that training data were mostly controls, the optimizer actually did a good job finding parameters that maximize P(D|b0, b1)

```{r}
# prediction probabilities of the controls
summary(fit.p[data$Class == 0])
# prediction probabilities of the cases
(fit.p[data$Class == 1])
```

Also, prediction probabilities for all the cases were below .5. How to understand this? 

The model by taking mostly controls did not fit the cases well. In another word, model parameters were basically dominated by the data of the controls. The case data tried pulling the parameters to other directions but only helped so much.

It is conceivable that strong predictors also helps.

## What about adding 100 case

```{r}
data = rbind(bc0, bc1[sample(1:nrow(bc1), 100), ])
fit <- glm(Class ~ Cell.shape, family = "binomial", data=data)
summary(fit)
(fit.p = predict(fit, type = "response")) %>% hist
table(fit.p > .5)
summary(fit.p[data$Class == 0])
summary(fit.p[data$Class == 1])
```

This is much better in that prediction probabilities of cases are much higher

Also note that model intercept is the over probability w/o any predictors

```{r}
fit <- glm(Class ~ 1, family = "binomial", data=data)
alpha = coef(fit)[1]
pfun(alpha)
mean(as.numeric(data$Class) - 1)
```

## Brief summary

Logistic regression analysis is mathematically finding parameter values that maximize the cost function. 
The parameter values are further transformed to P(1) for each subject by the sigmoid function that we introduced before.
This is the prediction probability (fit.p) for each subject.
These fit.p values can be interpreted as the event probability based on the given data, although the data might be highly imbalanced. 
These fit.p for a given subject will be shifted to the left or right depending on how many total cases in the training data, because model parameters were the best for all training data, but not specifically for cases or controls.
This is a probability. To use these probabilities for classification is a different thing, where a certain threshold is needed.
Consequently, different threshold yields different sensitivity and different specificity in terms of classification performance.
ROC and AU(ROC) show how specificity and sensitivity go with different thresholds, but these are metrics to score overall model performance, which itself does not determine the best threshold
REFS choose the threshold 0.5 as default. 

Accuracy, defined as the sum of sensitivity and specificity, is convenient metric to defind the best threshold, which accounts for both sensitivity and specificity.

```{r}
data = rbind(bc0, bc1[sample(1:nrow(bc1), 10), ])
fit <- glm(Class ~ Cell.shape, family = "binomial", data=data)
(fit.p = predict(fit, type = "response")) %>% hist

cutpoints = seq(0, 1, by = 0.01)

accuracy <- sapply(cutpoints, function(cut) {
  fit.y <- ifelse(fit.p > cut, 1, 0)
  fit.y <- factor(fit.y, levels=c(0, 1))
  mean(data$Class == fit.y) 
  
})

plot(cutpoints, accuracy, type = "b")
```

# The curve saturates quickly
# Accuracy curve of ATEZO results follow the same pattern, but saturated slower (around 0.5).
